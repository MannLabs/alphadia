{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '20'\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import neptune.new as neptune\n",
    "\n",
    "from alphabase.spectral_library.flat import SpecLibFlat\n",
    "from alphabase.spectral_library.base import SpecLibBase\n",
    "from alphabase.spectral_library.reader import SWATHLibraryReader\n",
    "\n",
    "from alphadia.extraction.data import TimsTOFDIA\n",
    "from alphadia.extraction.planning import Plan, Workflow\n",
    "from alphadia.extraction.calibration import RunCalibration\n",
    "from alphadia.extraction.candidateselection import MS1CentricCandidateSelection\n",
    "from alphadia.extraction.scoring import fdr_correction, unpack_fragment_info, MS2ExtractionWorkflow\n",
    "import alphadia.extraction.utils as utils\n",
    "yaml_file = 'config.yaml'\n",
    "\n",
    "raw_files = ['/Users/georgwallmann/Documents/data/raw_data/synchro_PASEF_4prot/20220923_TIMS03_PaSk_SA_4prot_HeLa_Evo05_21min_IM0713_200fmol_classical_SyS_4MS_wCE_S4-C3_1_32275.d']\n",
    "\n",
    "output_location = '/Users/georgwallmann/Documents/data/testing/2_feature_dev'\n",
    "\n",
    "try:\n",
    "    neptune_token = os.environ['NEPTUNE_TOKEN']\n",
    "except KeyError:\n",
    "    logging.error('NEPTUNE_TOKEN environtment variable not set')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lib = SpecLibBase()\n",
    "test_lib_location = '/Users/georgwallmann/Documents/data/raw_data/synchro_PASEF_4prot/dda_psms_maxquant/alpha_lib.hdf'\n",
    "test_lib.load_hdf(test_lib_location, load_mod_seq=True)\n",
    "\n",
    "plan = Plan(raw_files)\n",
    "plan.from_spec_lib_base(test_lib)\n",
    "\n",
    "for dia_data, precursors_flat, fragments_flat in plan.get_run_data():\n",
    "\n",
    "    raw_name = precursors_flat.iloc[0]['raw_name']\n",
    "\n",
    "    workflow = Workflow(\n",
    "        plan.config, \n",
    "        dia_data, \n",
    "        precursors_flat, \n",
    "        fragments_flat\n",
    "        )\n",
    "    \n",
    "    workflow.calibration()\n",
    "    df = workflow.extraction(keep_decoys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphadia.extraction import candidateselection\n",
    "\n",
    "selection = candidateselection.MS1CentricCandidateSelection(\n",
    "    dia_data,\n",
    "    precursors_flat,\n",
    "    rt_tolerance=15,\n",
    "    mz_tolerance=15,\n",
    "    mobility_tolerance=0.02,\n",
    "    candidate_count=3,\n",
    "    thread_count=1,\n",
    "    precursor_mz_column='mz_calibrated',\n",
    "    rt_column='rt_calibrated',\n",
    "    mobility_column='mobility_calibrated',\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "candidates = selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphadia.extraction import quadrupole, scoring, features\n",
    "q = quadrupole.SimpleQuadrupole(dia_data.cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extraction = scoring.MS2ExtractionWorkflow(\n",
    "    dia_data,\n",
    "    precursors_flat,\n",
    "    fragments_flat,\n",
    "    candidates,\n",
    "    q,\n",
    "    precursor_mz_tolerance=15,\n",
    "    fragment_mz_tolerance=20,\n",
    "    precursor_mz_column = 'mz_calibrated',\n",
    "    fragment_mz_column = 'mz_calibrated',\n",
    "    debug=False,\n",
    ")\n",
    "\n",
    "feature_df, fragment_df = extraction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df['correlation_score'] = feature_df[['fragment_frame_correlation', 'fragment_scan_correlation', 'template_frame_correlation', 'template_scan_correlation']].apply(np.mean, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df['precursor_idx'].nunique()\n",
    "df = scoring.fdr_correction(feature_df, feature_columns=\n",
    "            ['correlation_score'\n",
    "            ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df, x='correlation_score', hue='decoy', stat='count',  bins=100, element='step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='fragment_frame_correlation', y='fragment_scan_correlation', hue='decoy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphadia.extraction import plotting, quadrupole\n",
    "@nb.njit\n",
    "def build_feature(\n",
    "    dense_fragments,\n",
    "    template,\n",
    "    fragments\n",
    "\n",
    "):\n",
    "\n",
    "    total_fragment_intensity = np.sum(np.sum(dense_fragments[0], axis=-1), axis=-1)\n",
    "    total_template_intensity = np.sum(np.sum(template, axis=-1), axis=-1)\n",
    "\n",
    "    fragment_mask_2d = (total_fragment_intensity > 0).astype(np.int8)\n",
    "    fragment_mask_1d = np.sum(fragment_mask_2d, axis=-1) > 0\n",
    "    fragment_mask_2d = fragment_mask_2d * np.expand_dims(fragments.intensity, axis=-1)\n",
    "\n",
    "    # (n_fragments, n_observations, n_frames)\n",
    "    fragments_frame_profile = features.or_envelope_2d(features.frame_profile_2d(dense_fragments[0]))\n",
    "    template_frame_profile = features.or_envelope_2d(features.frame_profile_2d(template))\n",
    "\n",
    "    # (n_fragments, n_observations, n_scans)\n",
    "    fragments_scan_profile = features.or_envelope_2d(features.scan_profile_2d(dense_fragments[0]))\n",
    "    template_scan_profile = features.or_envelope_2d(features.scan_profile_2d(template))\n",
    "\n",
    "    \n",
    "\n",
    "    with nb.objmode:\n",
    "        plotting.plot_fragment_profile(\n",
    "            template,\n",
    "            fragments_scan_profile,\n",
    "            fragments_frame_profile,\n",
    "            template_frame_profile,\n",
    "            template_scan_profile,\n",
    "        )\n",
    "    \n",
    "\n",
    "    # (n_fragments, n_observations)\n",
    "    fragment_scan_correlation, template_scan_correlation = weighted_correlation(\n",
    "        fragments_scan_profile,\n",
    "        template_scan_profile,\n",
    "        fragment_mask_2d,\n",
    "    )\n",
    "\n",
    "    \n",
    "    # (n_fragments, n_observations)\n",
    "    fragment_frame_correlation, template_frame_correlation = weighted_correlation(\n",
    "        fragments_frame_profile,\n",
    "        template_frame_profile,\n",
    "        fragment_mask_2d,\n",
    "    )\n",
    "\n",
    "    observation_importance = quadrupole.calculate_observation_importance(template)\n",
    "    weights = fragments.intensity / np.sum(fragments.intensity)\n",
    "\n",
    "    fragment_scan_mean_list = np.sum(fragment_scan_correlation * observation_importance, axis = -1)\n",
    "    fragment_scan_mean_agg = np.sum(fragment_scan_mean_list * weights)\n",
    "    print(fragment_scan_mean_agg)\n",
    "\n",
    "    fragment_frame_mean_list = np.sum(fragment_frame_correlation * observation_importance, axis = -1)\n",
    "    fragment_frame_mean_agg = np.sum(fragment_frame_mean_list * weights)\n",
    "    print(fragment_frame_mean_agg)\n",
    "\n",
    "    template_scan_mean_list = np.sum(template_scan_correlation * observation_importance, axis = -1)\n",
    "    template_scan_mean_agg = np.sum(template_scan_mean_list * weights)\n",
    "    print(template_scan_mean_agg)\n",
    "\n",
    "    template_frame_mean_list = np.sum(template_frame_correlation * observation_importance, axis = -1)\n",
    "    template_frame_mean_agg = np.sum(template_frame_mean_list * weights)\n",
    "    print(template_frame_mean_agg)\n",
    "\n",
    "\n",
    "for candidate in candidate_container[:]:\n",
    "    if len(candidate.template) > 0:\n",
    "        res = build_feature(\n",
    "            candidate.dense_fragments,\n",
    "            candidate.template,\n",
    "            candidate.fragments\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.ones((5,5))\n",
    "mask = np.ones((5,5))\n",
    "np.fill_diagonal(mask,0)\n",
    "\n",
    "features.weighted_mean_a1(arr, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragment_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_fragments = candidate_container[8].dense_fragments[0]\n",
    "dense_fragments = dense_fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = candidate_container[8].template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "\n",
    "@nb.guvectorize([(nb.float64[:,:], nb.float64[:])], '(n, k)->(k)')\n",
    "def frame_profile(x, res):\n",
    "    res[:] = np.sum(x, axis=0)\n",
    "\n",
    "@nb.guvectorize([(nb.float64[:], nb.float64[:])], '(n)->()')\n",
    "def scan_profile(x, res):\n",
    "    res[0] = np.sum(x)\n",
    "\n",
    "@nb.guvectorize([\n",
    "    (nb.float64[:], nb.float64[:]),\n",
    "    (nb.float32[:], nb.float32[:]),\n",
    "    ], '(n)->(n)')\n",
    "def or_envelope(x, res):\n",
    "    res[:] = x\n",
    "    for i in range(1, len(x) - 1):\n",
    "        if (x[i] < x[i-1]) or (x[i] < x[i+1]):\n",
    "            res[i] = (x[i-1] + x[i+1]) / 2\n",
    "\n",
    "@nb.njit\n",
    "def frame_profile_2d(x):\n",
    "    return np.sum(x, axis=2)\n",
    "\n",
    "@nb.njit\n",
    "def frame_profile_1d(x):\n",
    "    return np.sum(x, axis=1)\n",
    "\n",
    "@nb.njit\n",
    "def scan_profile_2d(x):\n",
    "    return np.sum(x, axis=3)\n",
    "\n",
    "@nb.njit\n",
    "def scan_profile_1d(x):\n",
    "    return np.sum(x, axis=2)\n",
    "\n",
    "@nb.njit\n",
    "def or_envelope_1d(x):\n",
    "    res = x.copy()\n",
    "    for a0 in range(x.shape[0]):\n",
    "        for i in range(1, x.shape[1] - 1):\n",
    "            if (x[a0, i] < x[a0, i-1]) or (x[a0, i] < x[a0, i+1]):\n",
    "                res[a0, i] = (x[a0, i-1] + x[a0, i+1]) / 2\n",
    "\n",
    "@nb.njit\n",
    "def or_envelope_2d(x):\n",
    "    res = x.copy()\n",
    "    for a0  in range(x.shape[0]):\n",
    "        for a1 in range(x.shape[1]):\n",
    "            for i in range(1, x.shape[2] - 1):\n",
    "                if (x[a0, a1, i] < x[a0, a1, i-1]) or (x[a0, a1, i] < x[a0, a1, i+1]):\n",
    "                    res[a0, a1, i] = (x[a0, a1, i-1] + x[a0, a1, i+1]) / 2\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments_frame_profile = or_envelope_2d(frame_profile_2d(dense_fragments))\n",
    "fragments_scan_profile = or_envelope_2d(scan_profile_2d(dense_fragments))\n",
    "\n",
    "template_frame_profile = or_envelope_2d(frame_profile_2d(template))\n",
    "template_scan_profile = or_envelope_2d(scan_profile_1d(template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments_scan_profile[:,i_observations].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (n_fragments, n_observations)\n",
    "total_fragment_intensity = np.sum(np.sum(dense_fragments, axis=-1), axis=-1)\n",
    "fragment_mask_2d = (total_fragment_intensity > 0).astype(np.int8)\n",
    "fragment_mask_2d = fragment_mask_2d * np.expand_dims(candidate_container[8].fragments.intensity, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragment_mask_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "weighted_correlation(\n",
    "    fragments_frame_profile,\n",
    "    template_frame_profile,\n",
    "    #np.ones_like(fragment_mask_2d),\n",
    "    fragment_mask_2d,\n",
    ")\n",
    "#fragment_mask_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_2d = fragment_mask_2d[:,[0]]*fragment_mask_2d[:,0]\n",
    "np.fill_diagonal(mask_2d, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_mean_a1(\n",
    "    np.array([[1,2,np.nan],[4,5,6]]),\n",
    "    np.array([[1,0,0],[1,1,1]])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fragments_frame_profile = or_envelope(frame_profile(dense_fragments))\n",
    "fragments_scan_profile = or_envelope(scan_profile(dense_fragments))\n",
    "\n",
    "precursors_frame_profile = or_envelope(frame_profile(template))\n",
    "precursors_scan_profile = or_envelope(scan_profile(template))\n",
    "\n",
    "template_scan_profile = or_envelope(scan_profile(template))\n",
    "template_frame_profile = or_envelope(frame_profile(template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments_frame_profile[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ nb.njit\n",
    "def mean_correlation(\n",
    "    fragment_profile,\n",
    "    template_profile\n",
    "):\n",
    "    correlation = np.corrcoef(fragment_profile, template_profile)\n",
    "    \n",
    "    #mean_frame_corr = utils.amean0(correlation[:-1,:-1])-1/len(correlation[:-1,:-1])\n",
    "\n",
    "    return correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_container[1].fragments.intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mean_correlation(fragments_frame_profile[:,0], template_frame_profile[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit\n",
    "def weighted_precursor_correlation(\n",
    "    fragment_profile,\n",
    "    weights,\n",
    "):\n",
    "    n_fragemts = fragment_profile.shape[0]\n",
    "    n_observations = fragment_profile.shape[1]\n",
    "\n",
    "    weights = weights / np.sum(weights)\n",
    "\n",
    "    weighted_correlation = np.zeros((n_fragemts, n_observations))\n",
    "\n",
    "    for i_observations in range(n_observations):\n",
    "        correlation = np.corrcoef(fragments_frame_profile[:,i_observations])\n",
    "        weighted_correlation[:,i_observations] = np.sum(correlation * weights, axis = 1)\n",
    "\n",
    "    return weighted_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation * weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_correlation * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphadia.extraction import utils, plotting\n",
    "correlations = utils.calculate_correlations(\n",
    "        template_scan_profile[:,0], \n",
    "        fragments_scan_profile[:,0]\n",
    ")\n",
    "\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_profile = np.sum(dense_fragments, axis = -1)\n",
    "scan_profile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(20).reshape(20)\n",
    "h(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_scan_profile = np.sum(template, axis=-1)\n",
    "fragment_scan_profile = np.sum(dense_fragments, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragment_scan_profile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template_scan_profile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df['precursor_mass_error'] = np.abs(feature_df['precursor_mass_error'])\n",
    "feature_df['rt_error'] = np.abs(feature_df['rt_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scoring.fdr_correction(feature_df)\n",
    "df = scoring.fdr_correction(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sig = df[df['qval'] <= 0.01]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ids = np.setdiff1d(candidates[candidates['decoy'] == 0]['precursor_idx'].unique(), ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "candidate_container = cc\n",
    "\n",
    "fragment_collection = {'precursor_idx': []}\n",
    "feature_collection = []\n",
    "\n",
    "# initialize fragment collection with empty lists\n",
    "for key in candidate_container[0].fragment_features.keys():\n",
    "    fragment_collection[key] = []\n",
    "\n",
    "for i, c in enumerate(tqdm(candidate_container)):\n",
    "\n",
    "    n = 0\n",
    "    for key, item in c.fragment_features.items():\n",
    "        \n",
    "        fragment_collection[key].append(item)\n",
    "        n = len(item)\n",
    "        \n",
    "    fragment_collection['precursor_idx'].append(np.repeat(c.precursor_idx[0], n))\n",
    "    \n",
    "\n",
    "    if i > 10:\n",
    "        break\n",
    "    #feature_collection.append(self._collect_candidate(c))\n",
    "\n",
    "for key, item in fragment_collection.items():\n",
    "    fragment_collection[key] = np.concatenate(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc[0].precursor_idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, item in fragment_collection.items():\n",
    "    print(key, item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_dict = {}\n",
    "\n",
    "for key in cc[0].fragment_features.keys():\n",
    "    collection_dict[key] = []\n",
    "\n",
    "for c in cc:\n",
    "    for key, item in c.fragment_features.items():\n",
    "        collection_dict[key].append(item)\n",
    "\n",
    "for key, item in cc[0].fragment_features.items():\n",
    "    collection_dict[key] = np.concatenate(collection_dict[key])\n",
    "\n",
    "df = pd.DataFrame(collection_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "\n",
    "@nb.njit\n",
    "def cosine_similarity_1d(template_intensity, fragments_intensity):\n",
    "\n",
    "    fragment_norm = np.sqrt(np.sum(np.power(fragments_intensity,2),axis=-1))\n",
    "    template_norm = np.sqrt(np.sum(np.power(template_intensity,2),axis=-1))\n",
    "\n",
    "    div = (fragment_norm * template_norm) + 0.0001\n",
    "\n",
    "    return np.sum(fragments_intensity * template_intensity,axis=-1) / div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_intensity = np.sum(np.sum(cc[0].template, axis=-1), axis=-1)\n",
    "fragments_intensity = np.sum(np.sum(cc[0].dense_fragments[0], axis=-1), axis=-1)\n",
    "\n",
    "fragment_mask_2d = fragments_intensity > 0\n",
    "fragment_mask_1d = np.sum(fragment_mask_2d, axis=-1) > 0\n",
    "\n",
    "print(fragment_mask_1d)\n",
    "\n",
    "score = cosine_similarity_1d(template_intensity, fragments_intensity[fragment_mask_1d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#template_dist = template_dist / np.sum(template_dist, axis=-1)\n",
    "template_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#fragments_dist = fragments_dist/np.sum(fragments_dist, axis=-1, keepdims=True)\n",
    "\n",
    "fragments_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments_flat[['mz_library']].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = \n",
    "dot / "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit\n",
    "def cosine_similarity_int(a, b):\n",
    "    div = np.sqrt(np.sum(a))*np.sqrt(np.sum(b))\n",
    "    if div == 0:\n",
    "        return 0\n",
    "    return np.sum((a*b))/div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='base_width_mobility')[['base_width_mobility', 'precursor_idx','decoy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('base_width_mobility')[['precursor_idx','elution_group_idx','base_width_mobility','decoy']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphadia.extraction import scoring\n",
    "df = scoring.fdr_correction(df)\n",
    "\n",
    "\n",
    "df['significant'] = df['qval'] <= 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mean_observation_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['n_observations'] == 1]\n",
    "df[['mean_observation_score','precursor_idx','decoy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df, x='rt_error',hue='decoy', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=df, y='mean_observation_score', x='n_observations', hue='decoy', split=True,bw=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='mean_observation_score', y='var_observation_score', hue='decoy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sig = df[df['qval'] < 0.05]\n",
    "utils.density_scatter(df_sig['rt_library'].values, df_sig['rt_error'].values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "\n",
    "\n",
    "\n",
    "expand_cycle(dia_data.cycle,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfa83213b3107409f6f749b3f3e6e3a0c51921f6998a582a53d8fef6c3fba7c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
