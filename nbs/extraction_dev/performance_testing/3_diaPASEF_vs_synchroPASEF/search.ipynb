{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, psutil\n",
    "\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '20'\n",
    "from alphadia.extraction import processlogger\n",
    "processlogger.init_logging()\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import neptune.new as neptune\n",
    "\n",
    "from alphabase.spectral_library.flat import SpecLibFlat\n",
    "from alphabase.spectral_library.base import SpecLibBase\n",
    "from alphabase.spectral_library.reader import SWATHLibraryReader\n",
    "\n",
    "from alphadia.extraction.data import TimsTOFDIA\n",
    "from alphadia.extraction.planning import Plan, Workflow\n",
    "from alphadia.extraction.calibration import RunCalibration\n",
    "from alphadia.extraction.candidateselection import MS1CentricCandidateSelection\n",
    "from alphadia.extraction.scoring import fdr_correction, MS2ExtractionWorkflow\n",
    "import alphadia.extraction.utils as utils\n",
    "yaml_file = 'config.yaml'\n",
    "\n",
    "raw_files = ['/Users/georgwallmann/Documents/data/raw_data/Alpha_dia_benchmarking/diaPASEF_vs_synchroPASEF/20221221_TIMS05_PaSk_SA_HeLa_Evo05_200ng_21min_IM0713_diaPASEF_S4-A1_1_500.d',\n",
    "             #'/Users/georgwallmann/Documents/data/raw_data/Alpha_dia_benchmarking/diaPASEF_vs_synchroPASEF/20221221_TIMS05_PaSk_SA_HeLa_Evo05_200ng_21min_IM0713_diaPASEF_S4-A2_1_504.d',\n",
    "             #'/Users/georgwallmann/Documents/data/raw_data/Alpha_dia_benchmarking/diaPASEF_vs_synchroPASEF/20221221_TIMS05_PaSk_SA_HeLa_Evo05_200ng_21min_IM0713_diaPASEF_S4-A3_1_508.d',\n",
    "             #'/Users/georgwallmann/Documents/data/raw_data/Alpha_dia_benchmarking/diaPASEF_vs_synchroPASEF/20221221_TIMS05_PaSk_SA_HeLa_Evo05_200ng_21min_IM0713_diaPASEF_S4-A4_1_512.d',\n",
    "             #'/Users/georgwallmann/Documents/data/raw_data/Alpha_dia_benchmarking/diaPASEF_vs_synchroPASEF/20221221_TIMS05_PaSk_SA_HeLa_Evo05_200ng_21min_IM0713_SyP_classical_5bins_S2-A1_1_449.d',\n",
    "             #'/Users/georgwallmann/Documents/data/raw_data/Alpha_dia_benchmarking/diaPASEF_vs_synchroPASEF/20221221_TIMS05_PaSk_SA_HeLa_Evo05_200ng_21min_IM0713_SyP_classical_5bins_S2-A2_1_453.d',\n",
    "             #'/Users/georgwallmann/Documents/data/raw_data/Alpha_dia_benchmarking/diaPASEF_vs_synchroPASEF/20221221_TIMS05_PaSk_SA_HeLa_Evo05_200ng_21min_IM0713_SyP_classical_5bins_S2-A3_1_457.d',\n",
    "             #'/Users/georgwallmann/Documents/data/raw_data/Alpha_dia_benchmarking/diaPASEF_vs_synchroPASEF/20221221_TIMS05_PaSk_SA_HeLa_Evo05_200ng_21min_IM0713_SyP_classical_5bins_S2-A4_1_464.d'\n",
    "\n",
    "             ]\n",
    "\n",
    "output_location = '/Users/georgwallmann/Documents/data/alphadia_runs/2023_02_12_diaPASEF_vs_synchroPASEF/48_fraction_msfragger/'\n",
    "\n",
    "try:\n",
    "    neptune_token = os.environ['NEPTUNE_TOKEN']\n",
    "except KeyError:\n",
    "    logger.error('NEPTUNE_TOKEN environtment variable not set')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lib = SpecLibBase()\n",
    "test_lib_location = '/Users/georgwallmann/Documents/data/raw_data/Alpha_dia_benchmarking/diaPASEF_vs_synchroPASEF/library_48_fractions_MSFragger.hdf'\n",
    "test_lib.load_hdf(test_lib_location, load_mod_seq=True)\n",
    "\n",
    "plan = Plan(raw_files)\n",
    "plan.from_spec_lib_base(test_lib)\n",
    "plan.run(output_location, keep_decoys=True, fdr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dia_data, precursors_flat, fragments_flat in plan.get_run_data():\n",
    "    \n",
    "    raw_name = precursors_flat.iloc[0]['raw_name']\n",
    "\n",
    "    workflow = Workflow(\n",
    "        plan.config, \n",
    "        dia_data, \n",
    "        precursors_flat, \n",
    "        fragments_flat,\n",
    "        )\n",
    "    \n",
    "    workflow.calibration()\n",
    "    df = workflow.extraction(keep_decoys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = workflow.extraction(keep_decoys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['decoy'] == 0)&(df['qval'] <= 0.01)]['proteins'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scored = fdr_correction(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scored[(df_scored['decoy'] == 0)&(df_scored['qval'] <= 0.01)]['precursor_idx'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphadia.extraction import candidateselection\n",
    "\n",
    "selection = candidateselection.MS1CentricCandidateSelection(\n",
    "    dia_data,\n",
    "    precursors_flat,\n",
    "    rt_tolerance=60,\n",
    "    mz_tolerance=15,\n",
    "    mobility_tolerance=0.02,\n",
    "    candidate_count=10,\n",
    "    thread_count=20,\n",
    "    precursor_mz_column='mz_calibrated',\n",
    "    rt_column='rt_calibrated',\n",
    "    mobility_column='mobility_calibrated',\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "candidates = selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphadia.extraction import quadrupole, scoring, features\n",
    "q = quadrupole.SimpleQuadrupole(dia_data.cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extraction = scoring.MS2ExtractionWorkflow(\n",
    "    dia_data,\n",
    "    precursors_flat,\n",
    "    fragments_flat,\n",
    "    candidates,\n",
    "    q,\n",
    "    precursor_mz_tolerance=15,\n",
    "    fragment_mz_tolerance=15,\n",
    "    precursor_mz_column = 'mz_calibrated',\n",
    "    fragment_mz_column = 'mz_calibrated',\n",
    "    rt_column = 'rt_calibrated',\n",
    "    mobility_column = 'mobility_calibrated',\n",
    "    debug=False,\n",
    ")\n",
    "\n",
    "feature_df, fragment_df = extraction()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df['precursor_idx'].nunique()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df['decoy'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=feature_df, x='rt_calibrated', y='mz_calibrated', hue='decoy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df[feature_df['decoy'] == 0]['precursor_idx'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top = feature_df[feature_df['fragment_coverage'] > 0.4]\n",
    "idx = df_top.groupby(['precursor_idx'])['mean_fragment_intensity'].idxmax()\n",
    "df_top = df_top.loc[idx]\n",
    "df_top[df_top['decoy'] == 0]['precursor_idx'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "feature_columns = ['precursor_mass_error', \n",
    "            'precursor_isotope_correlation', \n",
    "            'fraction_fragments', \n",
    "            'intensity_correlation',\n",
    "            'sum_precursor_intensity',\n",
    "            'sum_fragment_intensity',\n",
    "            'mean_fragment_intensity',\n",
    "            'mean_fragment_nonzero',\n",
    "            'rt_error',\n",
    "            'mobility_error',\n",
    "            'mean_observation_score',\n",
    "            'var_observation_score',\n",
    "            'fragment_frame_correlation', 'fragment_scan_correlation', 'template_frame_correlation', 'template_scan_correlation'\n",
    "            ]\n",
    "\n",
    "df_top = df_top.dropna().reset_index(drop=True).copy()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('GBC', MLPClassifier(hidden_layer_sizes=(50, 25, 5), max_iter=400, alpha=1, learning_rate='adaptive', learning_rate_init=0.001, early_stopping=True, tol=1e-6))\n",
    "])\n",
    "\n",
    "X = df_top[feature_columns].values\n",
    "y = df_top['decoy'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_test_proba = pipeline.predict_proba(X_test)[:,1]\n",
    "y_test_pred = np.round(y_test_proba)\n",
    "\n",
    "y_train_proba = pipeline.predict_proba(X_train)[:,1]\n",
    "y_train_pred = np.round(y_train_proba)\n",
    "\n",
    "df_top['proba'] = pipeline.predict_proba(X)[:,1]\n",
    "\n",
    "df_top = df_top.sort_values(['proba'], ascending=True)\n",
    "target_values = 1-df_top['decoy'].values\n",
    "decoy_cumsum = np.cumsum(df_top['decoy'].values)\n",
    "target_cumsum = np.cumsum(target_values)\n",
    "fdr_values = decoy_cumsum/target_cumsum\n",
    "\n",
    "df_top['qval'] = scoring.fdr_to_q_values(fdr_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top[(df_top['decoy'] == 0)&(df_top['qval'] < 0.01)]['precursor_idx'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feature_df[feature_columns].values\n",
    "feature_df['proba'] = pipeline.predict_proba(X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = feature_df.groupby(['precursor_idx'])['proba'].idxmin()\n",
    "rescored = feature_df.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scored = fdr_correction(rescored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scored[(df_scored['decoy'] == 0)&(df_scored['qval'] <= 0.01)]['precursor_idx'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scored[(df_scored['decoy'] == 0)&(df_scored['qval'] > 0.01)&(df_scored['fragment_coverage'] > 0.5)]['precursor_idx'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scored['category'] = df_scored['decoy'].map({0: 'target', 1: 'decoy'})\n",
    "\n",
    "# leave category as target if qval <0.01 change it to target_missed if qval > 0.01\n",
    "df_scored['category'] = df_scored.apply(lambda x: 'target_missed' if x['category'] == 'target' and x['proba'] > 0.4 else x['category'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_scored, x='rt_error', hue='category', element='step', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(df_scored[df_scored['category'] == 'target'], x='rt_library', y='rt_error', s=1, hue='category', hue_order=[ 'target_missed', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df['correlation_score'] = feature_df[['fragment_frame_correlation', 'fragment_scan_correlation', 'template_frame_correlation', 'template_scan_correlation']].apply(np.mean, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_re = scoring.fdr_correction(feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_re[df_re['decoy'] == 0]\n",
    "\n",
    "df['fragment_coverage'] = np.round(10*df['fragment_coverage'])/10\n",
    "df['sig'] = df['qval'] < 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_re = df_re[df_re['base_width_mobility'] <0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_re, x='base_width_mobility', hue='decoy', stat='count',  bins=100, element='step', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='fragment_frame_correlation', y='fragment_scan_correlation', hue='decoy', alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.03/1200 * 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphadia.extraction import plotting, quadrupole\n",
    "@nb.njit\n",
    "def build_feature(\n",
    "    dense_fragments,\n",
    "    template,\n",
    "    fragments\n",
    "\n",
    "):\n",
    "\n",
    "    total_fragment_intensity = np.sum(np.sum(dense_fragments[0], axis=-1), axis=-1)\n",
    "    total_template_intensity = np.sum(np.sum(template, axis=-1), axis=-1)\n",
    "\n",
    "    fragment_mask_2d = (total_fragment_intensity > 0).astype(np.int8)\n",
    "    fragment_mask_1d = np.sum(fragment_mask_2d, axis=-1) > 0\n",
    "    fragment_mask_2d = fragment_mask_2d * np.expand_dims(fragments.intensity, axis=-1)\n",
    "\n",
    "    # (n_fragments, n_observations, n_frames)\n",
    "    fragments_frame_profile = features.or_envelope_2d(features.frame_profile_2d(dense_fragments[0]))\n",
    "    template_frame_profile = features.or_envelope_2d(features.frame_profile_2d(template))\n",
    "\n",
    "    # (n_fragments, n_observations, n_scans)\n",
    "    fragments_scan_profile = features.or_envelope_2d(features.scan_profile_2d(dense_fragments[0]))\n",
    "    template_scan_profile = features.or_envelope_2d(features.scan_profile_2d(template))\n",
    "\n",
    "    \n",
    "\n",
    "    with nb.objmode:\n",
    "        plotting.plot_fragment_profile(\n",
    "            template,\n",
    "            fragments_scan_profile,\n",
    "            fragments_frame_profile,\n",
    "            template_frame_profile,\n",
    "            template_scan_profile,\n",
    "        )\n",
    "    \n",
    "\n",
    "    # (n_fragments, n_observations)\n",
    "    fragment_scan_correlation, template_scan_correlation = weighted_correlation(\n",
    "        fragments_scan_profile,\n",
    "        template_scan_profile,\n",
    "        fragment_mask_2d,\n",
    "    )\n",
    "\n",
    "    \n",
    "    # (n_fragments, n_observations)\n",
    "    fragment_frame_correlation, template_frame_correlation = weighted_correlation(\n",
    "        fragments_frame_profile,\n",
    "        template_frame_profile,\n",
    "        fragment_mask_2d,\n",
    "    )\n",
    "\n",
    "    observation_importance = quadrupole.calculate_observation_importance(template)\n",
    "    weights = fragments.intensity / np.sum(fragments.intensity)\n",
    "\n",
    "    fragment_scan_mean_list = np.sum(fragment_scan_correlation * observation_importance, axis = -1)\n",
    "    fragment_scan_mean_agg = np.sum(fragment_scan_mean_list * weights)\n",
    "    print(fragment_scan_mean_agg)\n",
    "\n",
    "    fragment_frame_mean_list = np.sum(fragment_frame_correlation * observation_importance, axis = -1)\n",
    "    fragment_frame_mean_agg = np.sum(fragment_frame_mean_list * weights)\n",
    "    print(fragment_frame_mean_agg)\n",
    "\n",
    "    template_scan_mean_list = np.sum(template_scan_correlation * observation_importance, axis = -1)\n",
    "    template_scan_mean_agg = np.sum(template_scan_mean_list * weights)\n",
    "    print(template_scan_mean_agg)\n",
    "\n",
    "    template_frame_mean_list = np.sum(template_frame_correlation * observation_importance, axis = -1)\n",
    "    template_frame_mean_agg = np.sum(template_frame_mean_list * weights)\n",
    "    print(template_frame_mean_agg)\n",
    "\n",
    "\n",
    "for candidate in candidate_container[:]:\n",
    "    if len(candidate.template) > 0:\n",
    "        res = build_feature(\n",
    "            candidate.dense_fragments,\n",
    "            candidate.template,\n",
    "            candidate.fragments\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.ones((5,5))\n",
    "mask = np.ones((5,5))\n",
    "np.fill_diagonal(mask,0)\n",
    "\n",
    "features.weighted_mean_a1(arr, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragment_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_fragments = candidate_container[8].dense_fragments[0]\n",
    "dense_fragments = dense_fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = candidate_container[8].template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "\n",
    "@nb.guvectorize([(nb.float64[:,:], nb.float64[:])], '(n, k)->(k)')\n",
    "def frame_profile(x, res):\n",
    "    res[:] = np.sum(x, axis=0)\n",
    "\n",
    "@nb.guvectorize([(nb.float64[:], nb.float64[:])], '(n)->()')\n",
    "def scan_profile(x, res):\n",
    "    res[0] = np.sum(x)\n",
    "\n",
    "@nb.guvectorize([\n",
    "    (nb.float64[:], nb.float64[:]),\n",
    "    (nb.float32[:], nb.float32[:]),\n",
    "    ], '(n)->(n)')\n",
    "def or_envelope(x, res):\n",
    "    res[:] = x\n",
    "    for i in range(1, len(x) - 1):\n",
    "        if (x[i] < x[i-1]) or (x[i] < x[i+1]):\n",
    "            res[i] = (x[i-1] + x[i+1]) / 2\n",
    "\n",
    "@nb.njit\n",
    "def frame_profile_2d(x):\n",
    "    return np.sum(x, axis=2)\n",
    "\n",
    "@nb.njit\n",
    "def frame_profile_1d(x):\n",
    "    return np.sum(x, axis=1)\n",
    "\n",
    "@nb.njit\n",
    "def scan_profile_2d(x):\n",
    "    return np.sum(x, axis=3)\n",
    "\n",
    "@nb.njit\n",
    "def scan_profile_1d(x):\n",
    "    return np.sum(x, axis=2)\n",
    "\n",
    "@nb.njit\n",
    "def or_envelope_1d(x):\n",
    "    res = x.copy()\n",
    "    for a0 in range(x.shape[0]):\n",
    "        for i in range(1, x.shape[1] - 1):\n",
    "            if (x[a0, i] < x[a0, i-1]) or (x[a0, i] < x[a0, i+1]):\n",
    "                res[a0, i] = (x[a0, i-1] + x[a0, i+1]) / 2\n",
    "\n",
    "@nb.njit\n",
    "def or_envelope_2d(x):\n",
    "    res = x.copy()\n",
    "    for a0  in range(x.shape[0]):\n",
    "        for a1 in range(x.shape[1]):\n",
    "            for i in range(1, x.shape[2] - 1):\n",
    "                if (x[a0, a1, i] < x[a0, a1, i-1]) or (x[a0, a1, i] < x[a0, a1, i+1]):\n",
    "                    res[a0, a1, i] = (x[a0, a1, i-1] + x[a0, a1, i+1]) / 2\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments_frame_profile = or_envelope_2d(frame_profile_2d(dense_fragments))\n",
    "fragments_scan_profile = or_envelope_2d(scan_profile_2d(dense_fragments))\n",
    "\n",
    "template_frame_profile = or_envelope_2d(frame_profile_2d(template))\n",
    "template_scan_profile = or_envelope_2d(scan_profile_1d(template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments_scan_profile[:,i_observations].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (n_fragments, n_observations)\n",
    "total_fragment_intensity = np.sum(np.sum(dense_fragments, axis=-1), axis=-1)\n",
    "fragment_mask_2d = (total_fragment_intensity > 0).astype(np.int8)\n",
    "fragment_mask_2d = fragment_mask_2d * np.expand_dims(candidate_container[8].fragments.intensity, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragment_mask_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "weighted_correlation(\n",
    "    fragments_frame_profile,\n",
    "    template_frame_profile,\n",
    "    #np.ones_like(fragment_mask_2d),\n",
    "    fragment_mask_2d,\n",
    ")\n",
    "#fragment_mask_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_2d = fragment_mask_2d[:,[0]]*fragment_mask_2d[:,0]\n",
    "np.fill_diagonal(mask_2d, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_mean_a1(\n",
    "    np.array([[1,2,np.nan],[4,5,6]]),\n",
    "    np.array([[1,0,0],[1,1,1]])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fragments_frame_profile = or_envelope(frame_profile(dense_fragments))\n",
    "fragments_scan_profile = or_envelope(scan_profile(dense_fragments))\n",
    "\n",
    "precursors_frame_profile = or_envelope(frame_profile(template))\n",
    "precursors_scan_profile = or_envelope(scan_profile(template))\n",
    "\n",
    "template_scan_profile = or_envelope(scan_profile(template))\n",
    "template_frame_profile = or_envelope(frame_profile(template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments_frame_profile[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ nb.njit\n",
    "def mean_correlation(\n",
    "    fragment_profile,\n",
    "    template_profile\n",
    "):\n",
    "    correlation = np.corrcoef(fragment_profile, template_profile)\n",
    "    \n",
    "    #mean_frame_corr = utils.amean0(correlation[:-1,:-1])-1/len(correlation[:-1,:-1])\n",
    "\n",
    "    return correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_container[1].fragments.intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mean_correlation(fragments_frame_profile[:,0], template_frame_profile[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit\n",
    "def weighted_precursor_correlation(\n",
    "    fragment_profile,\n",
    "    weights,\n",
    "):\n",
    "    n_fragemts = fragment_profile.shape[0]\n",
    "    n_observations = fragment_profile.shape[1]\n",
    "\n",
    "    weights = weights / np.sum(weights)\n",
    "\n",
    "    weighted_correlation = np.zeros((n_fragemts, n_observations))\n",
    "\n",
    "    for i_observations in range(n_observations):\n",
    "        correlation = np.corrcoef(fragments_frame_profile[:,i_observations])\n",
    "        weighted_correlation[:,i_observations] = np.sum(correlation * weights, axis = 1)\n",
    "\n",
    "    return weighted_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation * weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_correlation * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphadia.extraction import utils, plotting\n",
    "correlations = utils.calculate_correlations(\n",
    "        template_scan_profile[:,0], \n",
    "        fragments_scan_profile[:,0]\n",
    ")\n",
    "\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_profile = np.sum(dense_fragments, axis = -1)\n",
    "scan_profile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(20).reshape(20)\n",
    "h(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_scan_profile = np.sum(template, axis=-1)\n",
    "fragment_scan_profile = np.sum(dense_fragments, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragment_scan_profile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template_scan_profile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df['precursor_mass_error'] = np.abs(feature_df['precursor_mass_error'])\n",
    "feature_df['rt_error'] = np.abs(feature_df['rt_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scoring.fdr_correction(feature_df)\n",
    "df = scoring.fdr_correction(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sig = df[df['qval'] <= 0.01]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ids = np.setdiff1d(candidates[candidates['decoy'] == 0]['precursor_idx'].unique(), ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "candidate_container = cc\n",
    "\n",
    "fragment_collection = {'precursor_idx': []}\n",
    "feature_collection = []\n",
    "\n",
    "# initialize fragment collection with empty lists\n",
    "for key in candidate_container[0].fragment_features.keys():\n",
    "    fragment_collection[key] = []\n",
    "\n",
    "for i, c in enumerate(tqdm(candidate_container)):\n",
    "\n",
    "    n = 0\n",
    "    for key, item in c.fragment_features.items():\n",
    "        \n",
    "        fragment_collection[key].append(item)\n",
    "        n = len(item)\n",
    "        \n",
    "    fragment_collection['precursor_idx'].append(np.repeat(c.precursor_idx[0], n))\n",
    "    \n",
    "\n",
    "    if i > 10:\n",
    "        break\n",
    "    #feature_collection.append(self._collect_candidate(c))\n",
    "\n",
    "for key, item in fragment_collection.items():\n",
    "    fragment_collection[key] = np.concatenate(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc[0].precursor_idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, item in fragment_collection.items():\n",
    "    print(key, item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_dict = {}\n",
    "\n",
    "for key in cc[0].fragment_features.keys():\n",
    "    collection_dict[key] = []\n",
    "\n",
    "for c in cc:\n",
    "    for key, item in c.fragment_features.items():\n",
    "        collection_dict[key].append(item)\n",
    "\n",
    "for key, item in cc[0].fragment_features.items():\n",
    "    collection_dict[key] = np.concatenate(collection_dict[key])\n",
    "\n",
    "df = pd.DataFrame(collection_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "\n",
    "@nb.njit\n",
    "def cosine_similarity_1d(template_intensity, fragments_intensity):\n",
    "\n",
    "    fragment_norm = np.sqrt(np.sum(np.power(fragments_intensity,2),axis=-1))\n",
    "    template_norm = np.sqrt(np.sum(np.power(template_intensity,2),axis=-1))\n",
    "\n",
    "    div = (fragment_norm * template_norm) + 0.0001\n",
    "\n",
    "    return np.sum(fragments_intensity * template_intensity,axis=-1) / div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_intensity = np.sum(np.sum(cc[0].template, axis=-1), axis=-1)\n",
    "fragments_intensity = np.sum(np.sum(cc[0].dense_fragments[0], axis=-1), axis=-1)\n",
    "\n",
    "fragment_mask_2d = fragments_intensity > 0\n",
    "fragment_mask_1d = np.sum(fragment_mask_2d, axis=-1) > 0\n",
    "\n",
    "print(fragment_mask_1d)\n",
    "\n",
    "score = cosine_similarity_1d(template_intensity, fragments_intensity[fragment_mask_1d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#template_dist = template_dist / np.sum(template_dist, axis=-1)\n",
    "template_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#fragments_dist = fragments_dist/np.sum(fragments_dist, axis=-1, keepdims=True)\n",
    "\n",
    "fragments_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments_flat[['mz_library']].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = \n",
    "dot / "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit\n",
    "def cosine_similarity_int(a, b):\n",
    "    div = np.sqrt(np.sum(a))*np.sqrt(np.sum(b))\n",
    "    if div == 0:\n",
    "        return 0\n",
    "    return np.sum((a*b))/div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='base_width_mobility')[['base_width_mobility', 'precursor_idx','decoy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('base_width_mobility')[['precursor_idx','elution_group_idx','base_width_mobility','decoy']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphadia.extraction import scoring\n",
    "df = scoring.fdr_correction(df)\n",
    "\n",
    "\n",
    "df['significant'] = df['qval'] <= 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mean_observation_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['n_observations'] == 1]\n",
    "df[['mean_observation_score','precursor_idx','decoy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df, x='rt_error',hue='decoy', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=df, y='mean_observation_score', x='n_observations', hue='decoy', split=True,bw=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='mean_observation_score', y='var_observation_score', hue='decoy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sig = df[df['qval'] < 0.05]\n",
    "utils.density_scatter(df_sig['rt_library'].values, df_sig['rt_error'].values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "\n",
    "\n",
    "\n",
    "expand_cycle(dia_data.cycle,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "pythonjvsc74a57bd0bfa83213b3107409f6f749b3f3e6e3a0c51921f6998a582a53d8fef6c3fba7c5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:01:00) \n[Clang 13.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfa83213b3107409f6f749b3f3e6e3a0c51921f6998a582a53d8fef6c3fba7c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
