# configuration for the extraction plan
version: 1

# These values are typically filled via CLI parameters
workflow_name: null
output_directory: null
library_path: null  # the expected type is steered by general.input_library_type
raw_paths: []
fasta_paths: []
quant_directory: null

general:

  thread_count: 10
  # Enables separate transfer learning step
  transfer_step_enabled: false

  # Enables separate MBR step
  mbr_step_enabled: false

  # === advanced settings ===
  # whether to reuse previously calculated calibration data
  reuse_calibration: false
  # whether to reuse previously calculated quantification data
  reuse_quant: false
  # whether to use Astral MS1 feature detection
  astral_ms1: false
  # logging verbosity level (DEBUG, INFO, PROGRESS, WARNING, ERROR)
  log_level: 'INFO'
  # whether to memory map detector events for improved performance
  mmap_detector_events: false
  # whether to use GPU acceleration for library prediction and scoring
  use_gpu: true
  # whether to save plots to the output folder
  save_figures: True

  # whether to save the pre-processed input library to the output directory
  save_library: True
  save_flat_library: False
  # type of input library, either 'base' or 'flat'
  # A 'flat' library can be used 'as is', whereas a 'base' library needs to be processed first.
  # Using 'flat' here can save some time during startup, which is handy for debugging and single-file runs.
  input_library_type: base

  # whether to save the output (mbr) library to the output directory
  save_mbr_library: True

  # if set to a positive integer, it will be used as random state (for reproducibility)
  # if set to -1, a random state will be generated and printed to the log
  # if null, no specific setting of random states is performed
  random_state: null

library_loading:
  rt_heuristic: 180
  # if retention times are reported in absolute units, the rt_heuristic defines rt is interpreted as minutes or seconds

library_prediction:
  # Basic parameters
  # whether to use alphaPeptDeep to predict peptide properties
  enabled: false
  # enzyme used for in-silico digest
  enzyme: trypsin
  # fixed modifications for in-silico digest
  # Format: Modification@AminoAcid
  # Example: Carbamidomethyl@C
  fixed_modifications: 'Carbamidomethyl@C'
  # variable modifications for in-silico digest. Semicolon separated list
  # See https://github.com/MannLabs/alphabase/blob/c99c1ec7eb482745f6dae58a324658d6b6c4abf0/alphabase/constants/const_files/modification.tsv
  # Format: Modification@AminoAcid
  # Example: Oxidation@M;Acetyl@Protein_N-term
  variable_modifications: 'Oxidation@M;Acetyl@Protein_N-term'
  # maximum number of variable modifications per peptide
  max_var_mod_num: 2
  # number of missed cleavages allowed for in-silico digest
  missed_cleavages: 1
  # using tuples here as it makes interaction with the GUI easier
  # minimum and maximum number of amino acids for generated precursors
  precursor_len:
    - 7
    - 35
  # minimum and maximum charge states for generated precursors
  precursor_charge:
    - 2
    - 4
  # minimum and maximum m/z values for generated precursors
  precursor_mz:
    - 400
    - 1200
  # minimum and maximum m/z values for generated fragments
  fragment_mz:
    - 200
    - 2000

  # === advanced settings ===
  # normalized collision energy for fragment generation
  nce: 25.0

  # list of fragment types (see alphabase.peptide.fragment.FRAGMENT_TYPES for supported types)
  # Supported types are: a, b, c, x, y, z, b_modloss, y_modloss, b_H2O, y_H2O, b_NH3, y_NH3, c_lossH, z_addH
  fragment_types:
    - 'b'
    - 'y'

  # maximum charge state for predicted fragments
  max_fragment_charge: 2

  # instrument types as supported by peptdeep. Leave this as Lumos if you are not 100% sure you know what you are changing
  instrument: Lumos

  # set path for custom peptdeep model. If set to null, the default model will be used
  peptdeep_model_path: null

  # set peptdeep model type. Possible values are 'generic', 'phospho', 'digly'.
  peptdeep_model_type: 'generic'

# define custom alphabase modifications not part of unimod or alphabase
# also used for decoy channels
custom_modifications:
  # Dimethyl @K channel decoy
  - name: Dimethyl:d12@K
    composition: H(-2)2H(8)13C(2)

  # Dimethyl @Any_N-term channel decoy
  - name: Dimethyl:d12@Any_N-term
    composition: H(-2)2H(8)13C(2)

  # Dimethyl @Protein_N-term channel decoy
  - name: Dimethyl:d12@Protein_N-term
    composition: H(-2)2H(8)13C(2)

  # mTRAQ @K channel decoy
  - name: mTRAQ:d12@K
    composition: H(12)C(1)13C(10)15N(2)O(1)

  # mTRAQ @Any_N-term channel decoy
  - name: mTRAQ:d12@Any_N-term
    composition: H(12)C(1)13C(14)15N(2)O(1)

  # mTRAQ @Protein_N-term channel decoy
  - name: mTRAQ:d12@Protein_N-term
    composition: H(12)C(1)13C(14)15N(2)O(1)

  # SILAC heavy @K channel decoy
  - name: Label:13C(12)@K
    composition: C(12)

search:
  # which backend to be used for the extraction. Current only option: classic
  extraction_backend: classic
  # target ms1 tolerance in ppm
  target_ms1_tolerance: 5
  # target ms2 tolerance in ppm
  target_ms2_tolerance: 10
  # target retention time tolerance in seconds if > 1, or a proportion of the total gradient length if < 1. 0.0 means to optimize automatically
  target_rt_tolerance: 0.0 # default is to optimize automatically
    # target ion mobility tolerance in 1/K_0
  target_mobility_tolerance: 0.0 # default is to optimize automatically

  # === advanced settings ===
  quant_window: 3
  target_num_candidates: 3
  # filter to apply to the channels. If set to "", all channels will be used.
  channel_filter: ""
  exclude_shared_ions: True
  compete_for_fragments: True
  # target retention time tolerance in seconds if > 1, or a proportion of the total gradient length if < 1
  quant_all: True

  # the number of most intense fragments to use in the selection
  top_k_fragments_selection: 12

  # the number of most intense fragments to use in the scoring
  top_k_fragments_scoring: 12

  # the minimum intensity of a fragment to be used in the search
  min_fragment_intensity: 0.01

  experimental_xic: True
  # use a more aggressive peak group score: benefits on max memory and runtime, but a small precursor penalty
  optimized_peak_group_score: False

calibration:

  # Number of precursors searched and scored per batch
  batch_size: 8000

  # minimum number of precursors to be found before search parameter optimization begins
  optimization_lock_target: 200

  # the maximum number of steps that a given optimizer is permitted to take
  max_steps: 20

  # the minimum number of steps that a given optimizer must take before it can be said to have converged
  min_steps: 2

  # the maximum number of times an automatic optimizer can be skipped before it is considered to have converged
  max_skips: 1

  # the maximum number of fragments with correlation scores exceeding correlation_threshold to use for calibrating fragment mz (i.e. ms2)
  max_fragments: 5000

  # the correlation threshold for fragments used to calibrate fragment mz (i.e. ms2)
  min_correlation: 0.7

search_initial:
  # Number of peak groups identified in the convolution score to classify with target decoy competition
  initial_num_candidates: 1

  # initial ms1 tolerance in ppm
  initial_ms1_tolerance: 30

  # initial ms2 tolerance in ppm
  initial_ms2_tolerance: 30

  # initial ion mobility tolerance in 1/K_0
  initial_mobility_tolerance: 0.1

  # initial retention time tolerance in seconds if > 1, or a proportion of the total gradient length if < 1
  initial_rt_tolerance: 0.5

# IMPORTANT NOTE: The parameters in this section are passed as kewords arguments to the CandidateSelectionConfig class
# Do not rename here without adapting that class!
selection_config:
  peak_len_rt: 10.
  sigma_scale_rt: 0.5
  peak_len_mobility: 0.01
  sigma_scale_mobility: 1.

  top_k_precursors: 3
  kernel_size: 30

  f_mobility: 1.0
  f_rt: 0.99
  center_fraction: 0.5
  min_size_mobility: 8
  min_size_rt: 3
  max_size_mobility: 20
  max_size_rt: 15

  group_channels: False
  use_weighted_score: True

  join_close_candidates: False
  join_close_candidates_scan_threshold: 0.6
  join_close_candidates_cycle_threshold: 0.6

# IMPORTANT NOTE: The parameters in this section are passed as kewords arguments to the CandidateScoringConfig class
# Do not rename here without adapting that class!
scoring_config:
  score_grouped: false
  top_k_isotopes: 3
  reference_channel: -1
  precursor_mz_tolerance: 10
  fragment_mz_tolerance: 15

# perform non-isobaric multiplexing of any input library
library_multiplexing:
  # if true, the library is multiplexed
  enabled: False

  # if the input library already contains multiplexed channels, the input channel has to be specified.
  input_channel: 0

  # define channels by their name and how modifications should be translated from the input library to the multiplexed library
  # channels can be either a number or a string
  # for every channel, the library gets copied and the modifications are translated according to the mapping
  # the following example shows how to multiplex mTRAQ to three sample channels and a decoy channel
  multiplex_mapping: []
#    - channel_name: 0
#      modifications:
#        mTRAQ@K: mTRAQ@K
#        mTRAQ@Any_N-term: mTRAQ@Any_N-term
#
#    - channel_name: 4
#      modifications:
#        mTRAQ@K: mTRAQ:13C(3)15N(1)@K
#        mTRAQ@Any_N-term: mTRAQ:13C(3)15N(1)@Any_N-term
#
#    - channel_name: 8
#      modifications:
#        mTRAQ@K: mTRAQ:13C(6)15N(2)@K
#        mTRAQ@Any_N-term: mTRAQ:13C(6)15N(2)@Any_N-term
#
#    - channel_name: 12
#      modifications:
#        mTRAQ@K: mTRAQ:d12@K
#        mTRAQ@Any_N-term: mTRAQ:d12@Any_N-term




multiplexing:
  enabled: False
  target_channels: '4,8'
  decoy_channel: 12
  reference_channel: 0
  competetive_scoring: True

fdr:
  fdr: 0.01
  group_level: 'proteins'
  inference_strategy: "heuristic"


  # === advanced settings ===
  competetive_scoring: true
  keep_decoys: false
  channel_wise_fdr: false

  # (Experimental)
  # uses a two-step classifier consisting of a logistic regression and a neural network, with a default maximum of 5 iterations per fitting call
  enable_two_step_classifier: false
  two_step_classifier_max_iterations: 5
  # (Experimental)
  # Optimizes the batch size and learning rate of the neural network
  enable_nn_hyperparameter_tuning: false

search_output:
  # Output file format for search results. Can be either "tsv" or "parquet"
  file_format: "tsv"
  # Enable label-free quantification at peptide level and generate peptide matrix
  peptide_level_lfq: false
  # Enable label-free quantification at precursor level and generate precursor matrix
  precursor_level_lfq: false

  # === advanced settings ===
  # Minimum number of fragments required for quantification
  min_k_fragments: 12
  # Minimum correlation required between fragment XICs for quantification
  min_correlation: 0.9
  # Number of samples used for quadratic fit in retention time alignment
  num_samples_quadratic: 50
  # Minimum number of non-missing values required for quantification
  min_nonnan: 3
  # Enable normalization of label-free quantification values
  normalize_lfq: True

  # Save fragment quant matrix for advanced users
  save_fragment_quant_matrix: False

# Configuration for the optimization of search parameters. These parameters should not normally be adjusted and are for the use of experienced users only.
optimization:
  # The order in which to perform optimization. Should be a list of lists of parameter names
  # Example:
    # order_of_optimization:
    #   - - "rt_error"
    #   - - "ms2_error"
    #   - - "ms1_error"
    #   - - "mobility_error"
  # The above means that first rt_error is optimized, then ms2_error, then ms1_error, and finally mobility_error. (Other examples are shown in Python list format rather than YAML format to save space.)
  # Example: [['ms1_error', 'ms2_error', 'rt_error', 'mobility_error']] means that all parameters are optimized simultaneously.
  # Example: [["ms2_error"], ["rt_error"], ["ms1_error"], ["mobility_error"]] means that the parameters are optimized sequentially in the order given.
  # Example: [["rt_error"], ["ms1_error", "ms2_error"]] means that first rt_error is optimized, then ms1_error and ms2_error are optimized simultaneously, and mobility_error is not optimized at all.
  # If order_of_optimization is null, first all targeted optimizers run simultaneously, then any remaining automatic optimizers run sequentially in the order [["ms2_error"], ["rt_error"], ["ms1_error"], ["mobility_error"]]
  order_of_optimization: null

  # Parameters for the update rule for each parameter:
  #   - update_percentile_range: the percentile interval to use (as a decimal)
  #   - update_factor: the factor by which to multiply the result from the percentile interval to get the new parameter value for the next round of search
  #   - try_narrower_parameters: if True, the optimization will try narrower parameters until a substantial (as determined by maximal_decrease) decrease in the feature used for optimization is observed.
  #   - maximal_decrease: the maximal decrease of the parameter value before stopping optimization (only relevant if favour_narrower_parameter is True).
  #     For example, a value of 0.2 indicates up to 20% decrease from the previous parameter is permissible.
  #   - favour_narrower_optimum: if True, the optimization will not take the value that maximizes the feature used for optimization, but instead the smallest value compatible with the maximum_decrease_from_maximum value.
  #     This setting can be useful for optimizing parameters for which many parameter values have similar feature values and therefore favouring narrower parameters helps to overcome noise.
  #   - maximum_decrease_from_maximum: the maximum proportional decrease from the maximum value of the parameter that the designated optimum should have (only relevant if favour_narrower_optimum is True).
  #     For example, a value of 0.1 indicates that the optimum should no more than 10% less than the maximum value.
  ms2_error:
      targeted_update_percentile_range: 0.95
      targeted_update_factor: 1.0
      automatic_update_percentile_range: 0.99
      automatic_update_factor: 1.1
      try_narrower_values: True
      maximal_decrease: 0.5
      favour_narrower_optimum: False
      maximum_decrease_from_maximum: 0.1
  ms1_error:
      targeted_update_percentile_range: 0.95
      targeted_update_factor: 1.0
      automatic_update_percentile_range: 0.99
      automatic_update_factor: 1.1
      try_narrower_values: False
      maximal_decrease: 0.2
      favour_narrower_optimum: False
      maximum_decrease_from_maximum: 0.1
  mobility_error:
      targeted_update_percentile_range: 0.95
      targeted_update_factor: 1.0
      automatic_update_percentile_range: 0.99
      automatic_update_factor: 1.1
      try_narrower_values: False
      maximal_decrease: 0.2
      favour_narrower_optimum: False
      maximum_decrease_from_maximum: 0.1
  rt_error:
      targeted_update_percentile_range: 0.95
      targeted_update_factor: 1.0
      automatic_update_percentile_range: 0.99
      automatic_update_factor: 1.1
      try_narrower_values: True
      maximal_decrease: 0.2
      favour_narrower_optimum: True
      maximum_decrease_from_maximum: 0.1

# configuration for the optimization manager
# initial parameters, will be optimized
optimization_manager:
  fwhm_rt: 5
  fwhm_mobility: 0.01
  score_cutoff: 0

# This section controls transfer learning
# currently only the library is created with transfer learning
transfer_library:
  # if true, the library is created for transfer learning
  enabled: False

  # === advanced settings ===

  # list of fragment types (see alphabase.peptide.fragment.FRAGMENT_TYPES for supported types)
  # Supported types are: a, b, c, x, y, z, b_modloss, y_modloss, b_H2O, y_H2O, b_NH3, y_NH3, c_lossH, z_addH
  fragment_types: ['b', 'y']

  # maximum charge for fragments
  max_charge: 2

  # If a given precursor appears multiple times in an experiment,
  # only the top_k_samples with the highest scores are included in the library
  top_k_samples: 3

  # (experimental) Perform advanced rt calibration:
  # If set to false retention times will be normalised by the maximum retention time observed in the experiment
  # If set to true, a combination of maximum normalisation and deviation from the calibration curve will be used
  norm_delta_max: true

  # Use only precursors for ms2 training with a median XIC correlation above this threshold
  precursor_correlation_cutoff: 0.5

  # include only fragments with a XIC correlation at least 0.75 of the median for all fragments
  fragment_correlation_ratio: 0.75

transfer_learning:

  # if true, a custom peptdeep model will be created using the transfer learned library
  enabled: False

  # === advanced settings ===

  # number of precursors per batch
  batch_size: 2000

  # maximum learning rate per batch.
  # The maximum learning rate will be reached after a warmup phase and decreased using a plateau scheduler
  max_lr: 0.0001

 # Fraction of dataset used for training
  train_fraction: 0.7

  # Fraction of dataset used for validation
  validation_fraction: 0.2

  # Fraction of dataset used for testing
  test_fraction: 0.1

  # test every n intervals
  test_interval: 1

  # learning rate patience after which the lr will be halved
  lr_patience: 3

  # maximum number of epochs
  epochs: 51

  # number of warmup epochs during which the lr is ramped up
  warmup_epochs: 5

  # normalised collision energy encoded during training
  nce: 25

  # instrument type encoded during training
  instrument: 'Lumos'
